{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Train.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HPICVAEbAewe","executionInfo":{"status":"ok","timestamp":1613395399038,"user_tz":-480,"elapsed":4469,"user":{"displayName":"王志远","photoUrl":"","userId":"13982724214817983181"}},"outputId":"c33a6294-46a0-4968-f771-7625f0afcb1b"},"source":["!pip install keras==2.1.2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting keras==2.1.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/89/58ee5f56a9c26957d97217db41780ebedca3154392cb903c3f8a08a52208/Keras-2.1.2-py2.py3-none-any.whl (304kB)\n","\r\u001b[K     |█                               | 10kB 23.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 15.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 30kB 13.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 51kB 10.2MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 61kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 71kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 81kB 11.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 92kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 102kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 112kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 122kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 133kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 143kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 153kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 163kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 174kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 184kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 194kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 204kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 215kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 225kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 235kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 245kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 256kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 266kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 276kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 286kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 296kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 307kB 9.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.2) (1.19.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.2) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.2) (1.4.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.2) (1.15.0)\n","\u001b[31mERROR: textgenrnn 1.4.1 has requirement keras>=2.1.5, but you'll have keras 2.1.2 which is incompatible.\u001b[0m\n","Installing collected packages: keras\n","  Found existing installation: Keras 2.4.3\n","    Uninstalling Keras-2.4.3:\n","      Successfully uninstalled Keras-2.4.3\n","Successfully installed keras-2.1.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H6n76AjCBj1x","executionInfo":{"status":"ok","timestamp":1613395441842,"user_tz":-480,"elapsed":47244,"user":{"displayName":"王志远","photoUrl":"","userId":"13982724214817983181"}},"outputId":"cb368ac7-3a31-4d81-f81a-6559516301f8"},"source":["## authorize access to google drive\r\n","from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hfar4zVDBz3w","executionInfo":{"status":"ok","timestamp":1613395441845,"user_tz":-480,"elapsed":47234,"user":{"displayName":"王志远","photoUrl":"","userId":"13982724214817983181"}},"outputId":"44472166-e819-4bcc-9c49-ab539e54a8ae"},"source":["# navigate to project directory\r\n","%cd '/content/drive/My Drive/Colab Notebooks/Proj'\r\n","# %cd '/content/drive/My Drive/Proj/'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/Proj\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0DPatxu4AjTL","executionInfo":{"status":"ok","timestamp":1613395454618,"user_tz":-480,"elapsed":12747,"user":{"displayName":"王志远","photoUrl":"","userId":"13982724214817983181"}},"outputId":"8a2c957e-9d02-4f65-faf1-c8a21e8b6a68"},"source":["\"\"\"\r\n","    解决：如果有100个类，如何划分，训练集、验证集和测试集。可以60个类训练、20个验证，最后20个测试。\r\n","\"\"\"\r\n","# import keras\r\n","# from keras_preprocessing import image\r\n","#import matplotlib.pyplot as plt\r\n","import numpy as np\r\n","import random\r\n","import tensorflow as tf\r\n","#import keras\r\n","#from keras import layers\r\n","# from keras_preprocessing import image\r\n","from tensorflow import keras\r\n","from tensorflow.keras import layers\r\n","#import tensorflow_datasets as tfds\r\n","import Func_ReadAllData\r\n","import ReadData\r\n","from tensorflow.keras.layers import Input, Conv1D, Activation, Dropout, MaxPooling1D, Conv2D, MaxPooling2D\r\n","import copy\r\n","\r\n","import os\r\n","import keras.backend.tensorflow_backend as KTF\r\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\r\n","config = tf.compat.v1.ConfigProto()\r\n","config.gpu_options.per_process_gpu_memory_fraction = 0.8\r\n","session = tf.compat.v1.Session(config=config)\r\n","KTF.set_session(session)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"MqBzKhLQAwQx"},"source":["\"\"\"\r\n","## Define the Hyperparameters\r\n","\"\"\"\r\n","learning_rate = 0.003 # 0.003\r\n","meta_step_size = 0.1 # 0.25\r\n","\r\n","inner_batch_size = 25\r\n","eval_batch_size = 25\r\n","\r\n","meta_iters = 2000  # 8000\r\n","eval_iters = 5\r\n","inner_iters = 4\r\n","\r\n","eval_interval = 1\r\n","train_shots = 20  # 20 5\r\n","shots = 20  # 5\r\n","\r\n","classes = 3  # 5\r\n","\r\n","\r\n","import readData\r\n","class Dataset:\r\n","    # This class will facilitate the creation of a few-shot dataset\r\n","    # from Collected data that can be sampled from quickly while also\r\n","    # allowing to create new labels at the same time.\r\n","    def __init__(self, training):\r\n","        self.data = {}\r\n","        data_train, labels_train = ReadData.loadData(startPath=\"split\",training=training)\r\n","        for data_tmp, label_tmp in zip(data_train, labels_train):\r\n","            data_tmp = np.expand_dims(data_tmp, axis=2)\r\n","            if label_tmp not in self.data:\r\n","                self.data[label_tmp] = []\r\n","            self.data[label_tmp].append(data_tmp)\r\n","            self.labels = list(self.data.keys())\r\n","\r\n","            \r\n","\r\n","    def get_mini_dataset(\r\n","        self, batch_size, repetitions, shots, num_classes, split=False, test=False\r\n","    ):\r\n","        testImage = {}\r\n","        testLabels = {}\r\n","        temp_labels = np.zeros(shape=(num_classes * shots))\r\n","        temp_images = np.zeros(shape=(num_classes * shots, 2000, 39, 1))\r\n","        if split:\r\n","            keys = np.zeros(shape=(num_classes))\r\n","            test_labels = np.zeros(shape=(num_classes))\r\n","            test_images = np.zeros(shape=(num_classes, 2000, 39, 1))\r\n","\r\n","        # Get a random subset of labels from the entire label set.\r\n","\r\n","        label_subset =[]\r\n","        label_subset = np.array(label_subset)\r\n","        tmpLabels = copy.deepcopy(self.labels)\r\n","        for i in range(num_classes):                                # num_classes 次不放回采样\r\n","            tmp = random.choices(tmpLabels,k=1)\r\n","            label_subset = np.concatenate((label_subset,tmp),axis=0)\r\n","            tmpLabels.remove(tmp[0])\r\n","        # label_subset = random.choices(self.labels, k=num_classes)  # k 次有放回采样\r\n","        for class_idx, class_obj in enumerate(label_subset):\r\n","            test = []\r\n","            if class_obj not in testImage:\r\n","              testImage[class_obj] = []\r\n","              testLabels[class_obj] = []\r\n","            # Use enumerated index value as a temporary label for mini-batch in\r\n","            # few shot learning.\r\n","            temp_labels[class_idx * shots : (class_idx + 1) * shots] = class_idx\r\n","\r\n","            # If creating a split dataset for testing, select an extra sample from each label to create the test dataset.\r\n","            if split:\r\n","                test_labels[class_idx] = class_idx\r\n","                images_to_split = random.choices(\r\n","                    self.data[label_subset[class_idx]], k=shots + 1   # 选出shot + 1个data\r\n","                )\r\n","\r\n","                test_images[class_idx] = images_to_split[-1]\r\n","                temp_images[\r\n","                    class_idx * shots : (class_idx + 1) * shots\r\n","                ] = images_to_split[:-1]\r\n","            else:\r\n","                # For each index in the randomly selected label_subset, sample the\r\n","                # necessary number of images.\r\n","                temp_images[\r\n","                    class_idx * shots : (class_idx + 1) * shots\r\n","                ] = random.choices(self.data[label_subset[class_idx]], k=shots)\r\n","\r\n","        dataset = tf.data.Dataset.from_tensor_slices(\r\n","            (temp_images.astype(np.float32), temp_labels.astype(np.int32))\r\n","        )\r\n","        dataset = dataset.shuffle(100).batch(batch_size).repeat(repetitions)  #打乱，喂入batch size，迭代repetitions次\r\n","        if split:\r\n","            return dataset, test_images, test_labels\r\n","        return dataset\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0L7aKEHd5eEY","executionInfo":{"elapsed":753677,"status":"ok","timestamp":1611825563690,"user":{"displayName":"Loki2 Wang","photoUrl":"","userId":"13723782343513414465"},"user_tz":-480},"outputId":"76ee15f0-2c76-434a-ec51-32c76ff26c43"},"source":["train_dataset = Dataset(training=True)\r\n","test_dataset = Dataset(training=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["personlist:['GZ', 'BiHongLiang', 'MZY', 'KuangRuiLin', 'HuHaiYan', 'LiLinWei', 'LPY', 'GQY', 'DengYangTao', 'HuangJiaQian', 'CZP', 'FCK', 'LT']\n","GZ\n"],"name":"stdout"},{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1dru5PCdCB6aT1Kb-75g8GYG37BEdecQR/Proj/ReadData.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  nda = np.array(data).shape[0]\n"],"name":"stderr"},{"output_type":"stream","text":["BiHongLiang\n","MZY\n","KuangRuiLin\n","HuHaiYan\n","LiLinWei\n","LPY\n","GQY\n","DengYangTao\n","HuangJiaQian\n","CZP\n","FCK\n","LT\n"],"name":"stdout"},{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1dru5PCdCB6aT1Kb-75g8GYG37BEdecQR/Proj/ReadData.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  dataValue = np.array(dataValue)\n","/content/drive/.shortcut-targets-by-id/1dru5PCdCB6aT1Kb-75g8GYG37BEdecQR/Proj/ReadData.py:63: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  labelsValue = np.array(labelsValue)\n"],"name":"stderr"},{"output_type":"stream","text":["personlist:['ZCY', 'ZJS', 'QinDang', 'XueMeng', 'ZhuTianLin', 'XZQ', 'ZhuXiaoTian', 'OuRunMin', 'WuYuan', 'WZY', 'ZJ', 'ZSF', 'ZJX', 'ZT']\n","ZCY\n","ZJS\n","QinDang\n","XueMeng\n","ZhuTianLin\n","XZQ\n","ZhuXiaoTian\n","OuRunMin\n","WuYuan\n","WZY\n","ZJ\n","ZSF\n","ZJX\n","ZT\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"VaovI9jEBHTK","executionInfo":{"elapsed":912,"status":"ok","timestamp":1612852722798,"user":{"displayName":"Loki2 Wang","photoUrl":"","userId":"13723782343513414465"},"user_tz":-480},"outputId":"2fd7ce59-01e1-4bd8-cfbe-df409bcb5314"},"source":["\"\"\"\r\n","## Build the model\r\n","\"\"\"\r\n","# def conv_bn(x):\r\n","#     x = layers.Conv2D(filters=64, kernel_size=3, strides=2, padding=\"same\")(x)\r\n","#     x = layers.BatchNormalization()(x)\r\n","#     return layers.ReLU()(x)\r\n","#\r\n","# inputs = layers.Input(shape=(2000, 39, 1))\r\n","# x = conv_bn(inputs)\r\n","# x = conv_bn(x)\r\n","# x = conv_bn(x)\r\n","# x = conv_bn(x)\r\n","# x = layers.Flatten()(x)\r\n","# outputs = layers.Dense(classes, activation=\"softmax\")(x)\r\n","# model = keras.Model(inputs=inputs, outputs=outputs)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n## Build the model\\n'"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"XvZb_qiuBCrB"},"source":["\"\"\"\r\n","## Change the model\r\n","\"\"\"\r\n","# X_input = Input(input_shape)\r\n","inputs = layers.Input(shape=(2000, 39, 1)) #, 1\r\n","X = Conv2D(filters=32, kernel_size=4, strides=1, name='conv0')(inputs) # 32\r\n","X = Activation('relu')(X)\r\n","# X = Dropout(0.3)(X)\r\n","X = MaxPooling2D(pool_size=2, strides=2, name='max_pool0')(X) #pool_size=2, strides=2 (4)\r\n","\r\n","X = Conv2D(256, 4, strides=1, name='conv1')(X)  # filter setting. 64\r\n","X = Activation('relu')(X)\r\n","# X = Dropout(0.3)(X)\r\n","X = MaxPooling2D(pool_size=2, strides=2, name='max_pool01')(X)\r\n","\r\n","X = Conv2D(128, 4, strides=1, name='conv11')(X)  # filter setting. #128\r\n","X = Activation('relu')(X)\r\n","# X = Dropout(0.3)(X)\r\n","X = MaxPooling2D(pool_size=2, strides=2, name='max_pool1')(X)\r\n","\r\n","X = Conv2D(64, 4, strides=1, name='conv2', padding=\"same\")(X)  # filter setting. # 256\r\n","X = Activation('relu')(X)\r\n","# X = Dropout(0.3)(X)\r\n","\r\n","X = MaxPooling2D(pool_size=2, strides=2, name='max_pool')(X) #2\r\n","X = layers.Flatten()(X)\r\n","X = layers.Dense(classes, activation='softmax', name='fc')(X)\r\n","model = keras.Model(inputs=inputs, outputs=X, name='VoiceChestModel_Keras')\r\n","\r\n","model.compile(metrics=['accuracy'])\r\n","optimizer = keras.optimizers.SGD(learning_rate=learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l3XgyFQCA3sx","executionInfo":{"elapsed":11917189,"status":"ok","timestamp":1611838386159,"user":{"displayName":"Loki2 Wang","photoUrl":"","userId":"13723782343513414465"},"user_tz":-480},"outputId":"5cf4d10a-6328-4c2b-9bfb-22ed7c805f9c"},"source":["\"\"\"\r\n","## Train the model\r\n","\"\"\"\r\n","# allData, allLabels = Func_ReadAllData.loadAllData(finalPath,1,training)\r\n","# allData = np.expand_dims(allData, axis=3)\r\n","# tmpAllLabels = []\r\n","# for label in allLabels:\r\n","#   tmpAllLabels.append(ord(label)-65)\r\n","\r\n","# tmpAllLabels = np.array(tmpAllLabels)\r\n","import os\r\n","training = []\r\n","testing = []\r\n","labelAcc = {}\r\n","labelSum = {}\r\n","count = 0\r\n","print(\"开始\")\r\n","# start_iter = 0\r\n","start_iter = len(training)\r\n","for meta_iter in range(1054,meta_iters):\r\n","  \r\n","  frac_done = meta_iter / meta_iters\r\n","  cur_meta_step_size = (1 - frac_done) * meta_step_size \r\n","\r\n","  # Temporarily save the weights from the model.\r\n","  old_vars = model.get_weights()\r\n","  # Get a sample from the full dataset. Train the few-shot.\r\n","  mini_dataset = train_dataset.get_mini_dataset(\r\n","      inner_batch_size, inner_iters, train_shots, classes\r\n","  )\r\n","\r\n","  for images, labels in mini_dataset: #？\r\n","    with tf.GradientTape() as tape:\r\n","      preds = model(images)\r\n","      loss = keras.losses.sparse_categorical_crossentropy(labels, preds)\r\n","    grads = tape.gradient(target=loss, sources=model.trainable_weights) \r\n","    optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n","  new_vars = model.get_weights()\r\n","  # Perform SGD for the meta step.  SGD：对每个样本进行更新\r\n","  for var in range(len(new_vars)):\r\n","    new_vars[var] = old_vars[var] + (\r\n","        (new_vars[var] - old_vars[var]) * cur_meta_step_size)\r\n","  \r\n","  # After the meta-learning step, reload the newly-trained weights into the model.\r\n","  model.set_weights(new_vars)\r\n","\r\n","\r\n","  # Evaluation loop\r\n","  if meta_iter % eval_interval == 0:\r\n","    accuracies = []\r\n","    for dataset in (train_dataset, test_dataset):\r\n","      count+=1\r\n","      # Sample a mini dataset from the full dataset.\r\n","      train_set, test_images, test_labels = dataset.get_mini_dataset(\r\n","        eval_batch_size, eval_iters, shots, classes, split=True)\r\n","      # print(len(train_set),len(test_images),len(test_labels))\r\n","      old_vars = model.get_weights()\r\n","      # Train on the samples and get the resulting accuracies.\r\n","      for images, labels in train_set:\r\n","        # print(len(images),len(labels))\r\n","        with tf.GradientTape() as tape:\r\n","          preds = model(images)\r\n","          loss = keras.losses.sparse_categorical_crossentropy(labels, preds)\r\n","        grads = tape.gradient(target=loss, sources=model.trainable_weights)\r\n","        optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n","      test_preds = model.predict(test_images)\r\n","      test_preds = tf.argmax(test_preds).numpy()\r\n","      # print(\"labels:{}, preds:{}\".format(test_labels,test_preds))\r\n","\r\n","      # test_loss, test_acc= model.evaluate(allData,tmpAllLabels)\r\n","      # print(test_loss,test_acc)\r\n","      \r\n","      num_correct = (test_preds == test_labels).sum()\r\n","      # Reset the weights after getting the evaluation accuracies.\r\n","      model.set_weights(old_vars)\r\n","      accuracies.append(num_correct / classes)\r\n","    training.append(accuracies[0])\r\n","    testing.append(accuracies[1])\r\n","\r\n","    if meta_iter % 10 == 0:\r\n","      \r\n","      print(\r\n","          \"batch %d: train=%f test=%f\" % (meta_iter, accuracies[0], accuracies[1])\r\n","      )\r\n","      np.save(os.path.join(\"checkpoint\",\"traing1.npy\"),training)\r\n","      np.save(os.path.join(\"checkpoint\",\"testing1.npy\"),testing)\r\n","\r\n","    #保存模型\r\n","    if meta_iter % 100 == 0:\r\n","      ModelName = os.path.join(\"checkpoint\",\"model1.h5\")\r\n","      model.save(ModelName)\r\n","    \r\n","    if meta_iter % 1999 == 0:\r\n","      ModelName = os.path.join(\"checkpoint\",\"model1.h5\")\r\n","      model.save(ModelName)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["开始\n","batch 1060: train=1.000000 test=1.000000\n","batch 1070: train=1.000000 test=1.000000\n","batch 1080: train=1.000000 test=1.000000\n","batch 1090: train=1.000000 test=1.000000\n","batch 1100: train=1.000000 test=1.000000\n","batch 1110: train=1.000000 test=1.000000\n","batch 1120: train=1.000000 test=1.000000\n","batch 1130: train=1.000000 test=1.000000\n","batch 1140: train=1.000000 test=1.000000\n","batch 1150: train=1.000000 test=1.000000\n","batch 1160: train=1.000000 test=1.000000\n","batch 1170: train=1.000000 test=1.000000\n","batch 1180: train=1.000000 test=0.333333\n","batch 1190: train=1.000000 test=1.000000\n","batch 1200: train=1.000000 test=1.000000\n","batch 1210: train=1.000000 test=1.000000\n","batch 1220: train=1.000000 test=1.000000\n","batch 1230: train=1.000000 test=1.000000\n","batch 1240: train=1.000000 test=1.000000\n","batch 1250: train=1.000000 test=1.000000\n","batch 1260: train=1.000000 test=1.000000\n","batch 1270: train=1.000000 test=1.000000\n","batch 1280: train=1.000000 test=1.000000\n","batch 1290: train=1.000000 test=0.666667\n","batch 1300: train=1.000000 test=1.000000\n","batch 1310: train=1.000000 test=0.333333\n","batch 1320: train=1.000000 test=1.000000\n","batch 1330: train=1.000000 test=1.000000\n","batch 1340: train=1.000000 test=1.000000\n","batch 1350: train=1.000000 test=1.000000\n","batch 1360: train=1.000000 test=1.000000\n","batch 1370: train=1.000000 test=1.000000\n","batch 1380: train=1.000000 test=1.000000\n","batch 1390: train=1.000000 test=1.000000\n","batch 1400: train=1.000000 test=1.000000\n","batch 1410: train=1.000000 test=1.000000\n","batch 1420: train=1.000000 test=1.000000\n","batch 1430: train=1.000000 test=1.000000\n","batch 1440: train=1.000000 test=1.000000\n","batch 1450: train=1.000000 test=1.000000\n","batch 1460: train=1.000000 test=1.000000\n","batch 1470: train=1.000000 test=1.000000\n","batch 1480: train=1.000000 test=1.000000\n","batch 1490: train=1.000000 test=1.000000\n","batch 1500: train=1.000000 test=1.000000\n","batch 1510: train=1.000000 test=1.000000\n","batch 1520: train=1.000000 test=1.000000\n","batch 1530: train=0.666667 test=1.000000\n","batch 1540: train=1.000000 test=1.000000\n","batch 1550: train=1.000000 test=1.000000\n","batch 1560: train=1.000000 test=1.000000\n","batch 1570: train=1.000000 test=1.000000\n","batch 1580: train=1.000000 test=1.000000\n","batch 1590: train=1.000000 test=1.000000\n","batch 1600: train=1.000000 test=1.000000\n","batch 1610: train=1.000000 test=1.000000\n","batch 1620: train=1.000000 test=0.333333\n","batch 1630: train=1.000000 test=1.000000\n","batch 1640: train=1.000000 test=1.000000\n","batch 1650: train=1.000000 test=1.000000\n","batch 1660: train=1.000000 test=1.000000\n","batch 1670: train=1.000000 test=1.000000\n","batch 1680: train=1.000000 test=1.000000\n","batch 1690: train=1.000000 test=1.000000\n","batch 1700: train=1.000000 test=1.000000\n","batch 1710: train=1.000000 test=1.000000\n","batch 1720: train=1.000000 test=1.000000\n","batch 1730: train=1.000000 test=1.000000\n","batch 1740: train=1.000000 test=1.000000\n","batch 1750: train=1.000000 test=1.000000\n","batch 1760: train=1.000000 test=1.000000\n","batch 1770: train=1.000000 test=1.000000\n","batch 1780: train=1.000000 test=1.000000\n","batch 1790: train=1.000000 test=1.000000\n","batch 1800: train=1.000000 test=1.000000\n","batch 1810: train=1.000000 test=1.000000\n","batch 1820: train=1.000000 test=1.000000\n","batch 1830: train=1.000000 test=1.000000\n","batch 1840: train=1.000000 test=1.000000\n","batch 1850: train=1.000000 test=1.000000\n","batch 1860: train=1.000000 test=0.666667\n","batch 1870: train=1.000000 test=1.000000\n","batch 1880: train=1.000000 test=1.000000\n","batch 1890: train=1.000000 test=1.000000\n","batch 1900: train=1.000000 test=1.000000\n","batch 1910: train=1.000000 test=1.000000\n","batch 1920: train=0.333333 test=1.000000\n","batch 1930: train=1.000000 test=1.000000\n","batch 1940: train=1.000000 test=1.000000\n","batch 1950: train=1.000000 test=1.000000\n","batch 1960: train=1.000000 test=1.000000\n","batch 1970: train=1.000000 test=1.000000\n","batch 1980: train=1.000000 test=1.000000\n","batch 1990: train=1.000000 test=1.000000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GdNIXjpSr72V"},"source":["# First, some preprocessing to smooth the training and testing arrays for display.\r\n","import matplotlib.pyplot as plt\r\n","window_length = 200\r\n","train_s = np.r_[\r\n","    training[window_length - 1 : 0 : -1], training, training[-1:-window_length:-1]\r\n","]\r\n","test_s = np.r_[\r\n","    testing[window_length - 1 : 0 : -1], testing, testing[-1:-window_length:-1]\r\n","]\r\n","w = np.hamming(window_length)\r\n","train_y = np.convolve(w / w.sum(), train_s, mode=\"valid\") # Numpy 中的卷积函数\r\n","test_y = np.convolve(w / w.sum(), test_s, mode=\"valid\")\r\n","\r\n","# Display the training accuracies.\r\n","x = np.arange(0, len(train_y), 1)\r\n","plt.plot(x, train_y, x, test_y)\r\n","plt.legend([\"train\",\"test\"])\r\n","plt.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VSJ8O2y_iJt3"},"source":["import copy\r\n","test_size = 20\r\n","class testDataset:\r\n","    # This class will facilitate the creation of a few-shot dataset\r\n","    # from Collected data that can be sampled from quickly while also\r\n","    # allowing to create new labels at the same time.\r\n","    def __init__(self, training):\r\n","        self.data = {}\r\n","        self.test_data = {}\r\n","        data_train, labels_train = ReadData.loadData(startPath=\"split\",training=training)\r\n","        data_test, labels_test = ReadData.loadData(startPath=os.path.join(\"Set2\"),training=training)\r\n","        for data_tmp, label_tmp in zip(data_train, labels_train):\r\n","            data_tmp = np.expand_dims(data_tmp, axis=2)\r\n","            if label_tmp not in self.data:\r\n","                self.data[label_tmp] = []\r\n","            self.data[label_tmp].append(data_tmp)\r\n","            self.labels = list(self.data.keys())\r\n","        for data_tmp, label_tmp in zip(data_test, labels_test):\r\n","            data_tmp = np.expand_dims(data_tmp, axis=2)\r\n","            if label_tmp not in self.test_data:\r\n","                self.test_data[label_tmp] = []\r\n","            self.test_data[label_tmp].append(data_tmp)\r\n","            self.test_labels = list(self.test_data.keys())\r\n","\r\n","            \r\n","\r\n","    def get_mini_dataset(\r\n","        self, batch_size, repetitions, shots, num_classes, split=False, test=False\r\n","    ):\r\n","        testImage = {}\r\n","        testLabels = {}\r\n","        temp_labels = np.zeros(shape=(num_classes * shots))\r\n","        temp_images = np.zeros(shape=(num_classes * shots, 2000, 39, 1))\r\n","        if split:\r\n","            keys = [0 for i in range(num_classes)]\r\n","            test_labels = np.zeros(shape=(num_classes))\r\n","            test_images = np.zeros(shape=(num_classes, 2000, 39, 1))\r\n","\r\n","        # Get a random subset of labels from the entire label set.\r\n","        # label_subset = random.choices(self.labels, k=num_classes)  # k 次有放回采样\r\n","        label_subset =[]\r\n","        label_subset = np.array(label_subset)\r\n","        tmpLabels = copy.deepcopy(self.labels)\r\n","        for i in range(num_classes):                                # num_classes 次不放回采样\r\n","            tmp = random.choices(tmpLabels,k=1)\r\n","            label_subset = np.concatenate((label_subset,tmp),axis=0)\r\n","            tmpLabels.remove(tmp[0])\r\n","        for class_idx, class_obj in enumerate(label_subset):\r\n","            if class_obj not in testImage:\r\n","              # testImage[class_obj] = []\r\n","              # testLabels[class_obj] = []\r\n","              testImage[class_obj] = np.zeros(shape=(test_size, 2000, 39, 1))\r\n","              testLabels[class_obj] = np.zeros(shape=(test_size))\r\n","            # Use enumerated index value as a temporary label for mini-batch in\r\n","            # few shot learning.\r\n","            temp_labels[class_idx * shots : (class_idx + 1) * shots] = class_idx\r\n","\r\n","            # If creating a split dataset for testing, select an extra sample from each label to create the test dataset.\r\n","            if split:\r\n","                # test_labels[class_idx] = class_idx\r\n","                test_labels[class_idx] = class_idx\r\n","                # images_to_split = random.choices(\r\n","                #     self.data[label_subset[class_idx]], k=shots + 1   # 选出shot + 1个data\r\n","                # )\r\n","                images_to_split = self.data[label_subset[class_idx]][0:shots+1]\r\n","                \r\n","                if test:\r\n","                  keys[class_idx] = class_obj\r\n","                  print(\"keys:{}\".format(keys))\r\n","\r\n","                  if(label_subset[class_idx]==\"HuangJiaQian\"):\r\n","                    print(label_subset[class_idx]+\"_device\")\r\n","                    testImage[class_obj] = random.choices(self.test_data[label_subset[class_idx]+\"_device\"],k=test_size)\r\n","                    testLabels[class_obj][0:] = class_idx \r\n","                  else:\r\n","                    testImage[class_obj] = random.choices(self.data[label_subset[class_idx]][shots+1:],k=test_size)\r\n","                    testLabels[class_obj][0:] = class_idx\r\n","                   \r\n","                  # testImage[class_obj] = random.choices(self.data[label_subset[class_idx]][shots+1:],k=test_size)\r\n","                  # testLabels[class_obj][0:] = class_idx \r\n","\r\n","                  \r\n","\r\n","                test_images[class_idx] = images_to_split[-1]\r\n","                temp_images[\r\n","                    class_idx * shots : (class_idx + 1) * shots\r\n","                ] = images_to_split[:-1]\r\n","            else:\r\n","                # For each index in the randomly selected label_subset, sample the\r\n","                # necessary number of images.\r\n","                temp_images[\r\n","                    class_idx * shots : (class_idx + 1) * shots\r\n","                ] = random.choices(self.data[label_subset[class_idx]], k=shots)\r\n","\r\n","        # print(keys)\r\n","        dataset = tf.data.Dataset.from_tensor_slices(\r\n","            (temp_images.astype(np.float32), temp_labels.astype(np.int32))\r\n","        )\r\n","        dataset = dataset.shuffle(100).batch(batch_size).repeat(repetitions)  #打乱，喂入batch size，迭代repetitions次\r\n","        if test:\r\n","          tmpImage = []\r\n","          tmpLabels = []\r\n","          # print(len(list(testLabels.values())[0]))\r\n","          l = len(list(testLabels.values())[0])\r\n","          # print(l)\r\n","          for i in range(l):\r\n","            for k in label_subset:\r\n","              # print(\"i:{},lemgth:{}\".format(i,len(list(testImage[k]))))\r\n","              # print(\"length:{},idx:{}\".format(len(list(testImage[k])),i))\r\n","              tmpImage.append(list(testImage[k])[i])\r\n","              \r\n","              tmpLabels.append(list(testLabels[k])[i])\r\n","          # print(len(tmpImage))\r\n","          tmpImage = np.array(tmpImage)\r\n","          tmpLabels = np.array(tmpLabels)\r\n","          testset = tf.data.Dataset.from_tensor_slices((tmpImage.astype(np.float32), tmpLabels.astype(np.float32)))\r\n","          testset = testset.batch(num_classes)\r\n","          keys = np.array(keys)\r\n","          return dataset, test_images, test_labels, keys, testset\r\n","        if split:\r\n","            return dataset, test_images, test_labels\r\n","        return dataset\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Yzyq-5myAAz","outputId":"b25bdc9d-6a30-4512-83d6-8eaa7dd7d326"},"source":["testSet = testDataset(False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["personlist:['ZCY', 'ZJS', 'QinDang', 'XueMeng', 'ZhuTianLin', 'XZQ', 'ZhuXiaoTian', 'WZY', 'ZJ', 'ZSF', 'ZT', 'LXR', 'LTM', 'GYY', 'MMH2', 'HNC']\n","ZCY\n"],"name":"stdout"},{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/Proj/ReadData.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  nda = np.array(data).shape[0]\n"],"name":"stderr"},{"output_type":"stream","text":["ZJS\n","QinDang\n","XueMeng\n","ZhuTianLin\n","XZQ\n","ZhuXiaoTian\n","WZY\n","ZJ\n","ZSF\n","ZT\n","LXR\n","LTM\n","GYY\n","MMH2\n","HNC\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GJ5meM12LmSF","executionInfo":{"status":"ok","timestamp":1613401240706,"user_tz":-480,"elapsed":16229,"user":{"displayName":"王志远","photoUrl":"","userId":"13982724214817983181"}},"outputId":"90a94802-ba33-465a-f5a0-865be01fdda2"},"source":["trainSet = testDataset(True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["personlist:['HuHaiYan', 'LiLinWei', 'LPY', 'GQY', 'HuangJiaQian', 'CZP', 'GZ', 'LT']\n","HuHaiYan\n"],"name":"stdout"},{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/Proj/ReadData.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  nda = np.array(data).shape[0]\n"],"name":"stderr"},{"output_type":"stream","text":["LiLinWei\n","LPY\n","GQY\n","HuangJiaQian\n","CZP\n","GZ\n","LT\n"],"name":"stdout"},{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/Proj/ReadData.py:67: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  dataValue = np.array(dataValue)\n","/content/drive/My Drive/Colab Notebooks/Proj/ReadData.py:68: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  labelsValue = np.array(labelsValue)\n"],"name":"stderr"},{"output_type":"stream","text":["personlist:['HuangJiaQian_device']\n","HuangJiaQian_device\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3rOoYQu7RMkA"},"source":["model = tf.keras.models.load_model(\"checkpoint/model1.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IZDZSYwHdZ--","outputId":"c1690d03-d768-49e4-82e2-59a5dc04e4b0"},"source":["number = 200\r\n","acc = {}\r\n","# for shots in range(1,21,1):\r\n","#   print(\"shots:{}\".format(shots))\r\n","shots=20\r\n","# test_size = 45\r\n","\r\n","all_count = {}  #保存各个类被抽取出的总次数\r\n","label_dic = {}  #保存各个类被抽取出来后预测正确的次数，与上面的总次数相除暂定为预测的准确度\r\n","for i in range(number):\r\n","  testing = []\r\n","  train_set, test_images, test_labels, keys, test_set = trainSet.get_mini_dataset(\r\n","        eval_batch_size, eval_iters, shots, classes, split=True, test=True\r\n","    )\r\n","  # print(test_set)\r\n","  for l in keys:\r\n","    if l not in label_dic:\r\n","      label_dic[l]=0\r\n","    if l not in all_count:\r\n","      all_count[l]=0\r\n","  old_vars1 = model.get_weights()\r\n","  # train\r\n","  for images, labels in train_set:\r\n","    # print(labels)\r\n","    with tf.GradientTape() as tape:\r\n","        preds = model(images)\r\n","        loss = keras.losses.sparse_categorical_crossentropy(labels, preds)\r\n","    # print(loss)\r\n","    # print(\"labels:{}, preds:{}\".format(labels,preds))\r\n","    grads = tape.gradient(loss, model.trainable_weights)\r\n","    optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n","  test_preds = model.predict(test_images)\r\n","  test_preds = tf.argmax(test_preds).numpy()\r\n","  # if(i%50==0):\r\n","  #   print(i)\r\n","  print(i)\r\n","  # print(\"labels:{}, preds:{}\".format(test_labels,test_preds))\r\n","  # test\r\n","  for images, labels in test_set:\r\n","      old_vars2 = model.get_weights()\r\n","      for label in labels:\r\n","        # print(label.numpy())\r\n","        all_count[keys[label.numpy().astype(int)]]+=1 \r\n","      preds = model.predict(images)\r\n","      preds = tf.argmax(preds).numpy()\r\n","      print(\"labels:{}, preds:{}\".format(labels,preds))\r\n","      flags = (labels==preds)\r\n","      num_correct = flags.numpy().sum()\r\n","      testing.append(num_correct/classes)\r\n","      for idx, flag in enumerate(flags):\r\n","        if flag:\r\n","          label_dic[keys[idx]]+=1\r\n","      model.set_weights(old_vars2)\r\n","  model.set_weights(old_vars1)\r\n","result = {}\r\n","for key in label_dic.keys():\r\n","  result[key] = (label_dic[key]/all_count[key])*100\r\n","  print(\"label:{},  抽取出来的总次数:{},  其中预测正确的次数:{},  acc:{:.2f}%\".format(key,all_count[key],label_dic[key],(label_dic[key]/all_count[key])*100))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["keys:['GZ', 0, 0]\n","keys:['GZ', 'LiLinWei', 0]\n","keys:['GZ', 'LiLinWei', 'CZP']\n","0\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","keys:['GQY', 0, 0]\n","keys:['GQY', 'CZP', 0]\n","keys:['GQY', 'CZP', 'HuHaiYan']\n","1\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","keys:['LPY', 0, 0]\n","keys:['LPY', 'HuHaiYan', 0]\n","keys:['LPY', 'HuHaiYan', 'LT']\n","2\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","keys:['LT', 0, 0]\n","keys:['LT', 'LPY', 0]\n","keys:['LT', 'LPY', 'CZP']\n","3\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n","labels:[0. 1. 2.], preds:[0 1 2]\n"],"name":"stdout"}]}]}